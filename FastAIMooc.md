#AWS
aws instance preinstalled
tmux terminal multiplexer
spot instances (cheap but do not sve on termination)

#Python
bcolz - fast and compact numpy array saving

#kaggle
kaggle cli

#Dynamic learning rate
1. RMS prop: divide learning rate by RMS of parameter gradient
1. Momentum: add weighted moving average of gradient to gradient direction
1. ADAM: RMSProp + Momentum
1. EVE: ADAM + learningrate annealing (difficult with stopping criterion)

#Input normalization
Start with a batch norm layer (Lesson 4 58:30)

#Avoiding overfitting
1. Add more data
1. Use data augmentation
1. Use architectures that generalize well
1. Add regularization
1. Reduce architecture complexity.

#Random
* Asking people about their behaviour is crap compared to looking at their behaviour

#TODO
1. Pseudolabeling (Geoffry Hinton) Distilling knowledge
1. Spearmint hyperparameter tuning
1. Latent factors
